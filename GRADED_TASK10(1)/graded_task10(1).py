# -*- coding: utf-8 -*-
"""GRADED_TASK10(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lTpK-bcCAk1SQh0NotLEThf-AjwitJLE
"""

docs = [
    "Internet security is the collection of practices, technologies, and concepts that shield any Internet-connected software or hardware we use from online threats.  The number of devices we use at home or work that can connect to a network is increasing rapidly. In addition to our laptops, desktops, servers, routers, modems, smartphones, and tablets, our watches, appliances, thermostats, doorbell cameras, and appliances are also Internet-connected devices.Internet connectivity allows our technology to be more useful and convenient, and much smarter. However, opening up the devices and software we rely on to the data superhighway also leaves them vulnerable to hackers, malware, exploits, bots, spam, malicious links and websites. Internet-connected technology is also more susceptible to social engineering attacks like phishing. And sophisticated threat actors can use devices in a network for lateral movements, data exfiltration, or to drop ransomware.With the increasing use of the Internet, cybercriminals have more opportunities to access sensitive data and cause financial losses. Internet security is essential to protect businesses, individuals, and governments from malicious attacks such as hacking, malware, ransomware, and identity theft. It also helps protect financial transactions and other sensitive information from being stolen or misused. Internet security measures can help prevent data breaches that can lead to costly damages for organizations and individuals alike..."
]

lines = []
for docs in docs:
    lines.extend(docs.split('. '))

for line in lines:
    print(line)

import spacy
# Creting List of Stop Words
from spacy.lang.en.stop_words import STOP_WORDS
stop_words = spacy.lang.en.stop_words.STOP_WORDS

# Creating list of punctuation marks
import string
punctuations = string.punctuation

print(stop_words)
print("\n===================\n")
print(punctuations)

nlp = spacy.load('en_core_web_sm')

prc_docs = [nlp(doc) for doc in docs]
print(prc_docs)

for tok in prc_docs[0]:
  print(tok)

# Document Representation before applying lemmatization
print("Before: ", prc_docs)


token_docs = [ [tok.lemma_.lower().strip() for tok in prc_doc] for prc_doc in prc_docs]

print("\nAfter: ", token_docs)

print("Before: ", token_docs)

token_docs = [ [tok for tok in token_doc if (tok not in stop_words and tok not in punctuations)] for token_doc in token_docs]
print("\nAfter: ",token_docs)

s = ''
docs = []
for token_doc in token_docs:
    docs.append(' '.join(token_doc))
print(docs)

import nltk
nltk.download('punkt')

from nltk.stem.porter import PorterStemmer

porter_stemmer  = PorterStemmer()

docs = ["artificial intelligence is impacting the world brilliantly.",
       "Python is the most commonly used language for Ai.",
       "AI solves many problems with ina seconds."]


tokenization = [nltk.word_tokenize(i) for i in docs]

docs_stemmer = []
for w in tokenization:
  for i in w:
    print(porter_stemmer.stem(i))

import nltk
from nltk import word_tokenize, pos_tag

nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Example sentence
sentence = "AI can replaces humans."

# Tokenize the sentence
tokens = word_tokenize(sentence)

# Perform POS tagging
pos_tags = pos_tag(tokens)

# Display the POS tags
print(pos_tags)